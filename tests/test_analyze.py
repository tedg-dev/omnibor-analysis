#!/usr/bin/env python3
"""
Tests for app/analyze.py — class-based analysis pipeline.

Uses unittest.mock to avoid real subprocess calls.
"""

import sys
import tempfile
import unittest
from pathlib import Path
from unittest.mock import MagicMock, patch

# Add app/ to path so we can import analyze
sys.path.insert(0, str(Path(__file__).parent.parent / "app"))

import analyze
from analyze import (
    CommandRunner, DependencyValidator,
    RepoCloner, BomtraceBuilder,
    SpdxGenerator, SpdxValidator,
    SyftGenerator, BinaryCollector, DocWriter,
    AnalysisPipeline, load_config, timestamp,
)


# ============================================================
# Utilities
# ============================================================

class TestLoadConfig(unittest.TestCase):
    """Tests for load_config()."""

    def test_loads_real_config(self):
        config = load_config()
        self.assertIn("repos", config)
        self.assertIn("paths", config)
        self.assertIn("omnibor", config)

    def test_loads_custom_path(self):
        import yaml
        with tempfile.NamedTemporaryFile(
            mode="w", suffix=".yaml", delete=False,
        ) as f:
            yaml.dump({"test": True}, f)
            tmp = Path(f.name)
        result = load_config(tmp)
        self.assertTrue(result["test"])
        tmp.unlink()


class TestTimestamp(unittest.TestCase):
    """Tests for timestamp()."""

    def test_format(self):
        ts = timestamp()
        self.assertRegex(
            ts, r"\d{4}-\d{2}-\d{2}_\d{4}"
        )


# ============================================================
# CommandRunner
# ============================================================

class TestCommandRunner(unittest.TestCase):
    """Tests for CommandRunner."""

    @patch("analyze.subprocess.run")
    def test_success(self, mock_run):
        mock_run.return_value = MagicMock(
            returncode=0, stdout="ok\n",
        )
        runner = CommandRunner()
        with patch("builtins.print"):
            rc = runner.run(
                "echo hello", description="test"
            )
        self.assertEqual(rc, 0)

    @patch("analyze.subprocess.run")
    def test_failure(self, mock_run):
        mock_run.return_value = MagicMock(
            returncode=1, stdout="fail\n",
        )
        runner = CommandRunner()
        with patch("builtins.print"):
            rc = runner.run(
                "false", description="fail test"
            )
        self.assertEqual(rc, 1)

    @patch("analyze.subprocess.run")
    def test_cwd_passed(self, mock_run):
        mock_run.return_value = MagicMock(
            returncode=0, stdout="",
        )
        runner = CommandRunner()
        with patch("builtins.print"):
            runner.run(
                "ls", cwd="/tmp",
                description="cwd test",
            )
        mock_run.assert_called_once()
        self.assertEqual(
            mock_run.call_args.kwargs.get("cwd")
            or mock_run.call_args[1].get("cwd"),
            "/tmp",
        )

    @patch("analyze.subprocess.run")
    def test_prints_error_on_failure(self, mock_run):
        mock_run.return_value = MagicMock(
            returncode=42, stdout="",
        )
        runner = CommandRunner()
        printed = []
        with patch(
            "builtins.print",
            side_effect=lambda *a, **kw: (
                printed.append(
                    " ".join(str(x) for x in a)
                )
            ),
        ):
            runner.run("bad", description="x")
        output = "\n".join(printed)
        self.assertIn("42", output)


# ============================================================
# DependencyValidator
# ============================================================

class TestDependencyValidator(unittest.TestCase):
    """Tests for DependencyValidator."""

    def test_no_apt_deps(self):
        runner = MagicMock()
        v = DependencyValidator(runner)
        ok, missing = v.validate({})
        self.assertTrue(ok)
        self.assertEqual(missing, [])
        runner.run.assert_not_called()

    def test_empty_apt_deps(self):
        runner = MagicMock()
        v = DependencyValidator(runner)
        ok, missing = v.validate({"apt_deps": []})
        self.assertTrue(ok)
        self.assertEqual(missing, [])

    def test_all_installed(self):
        runner = MagicMock()
        runner.run.return_value = 0
        v = DependencyValidator(runner)
        with patch("builtins.print"):
            ok, missing = v.validate(
                {"apt_deps": ["libssl-dev", "zlib1g-dev"]}
            )
        self.assertTrue(ok)
        self.assertEqual(missing, [])
        self.assertEqual(runner.run.call_count, 2)

    def test_some_missing(self):
        runner = MagicMock()
        runner.run.side_effect = [0, 1, 0]
        v = DependencyValidator(runner)
        with patch("builtins.print"):
            ok, missing = v.validate(
                {"apt_deps": [
                    "libssl-dev", "libpsl-dev",
                    "zlib1g-dev",
                ]}
            )
        self.assertFalse(ok)
        self.assertEqual(missing, ["libpsl-dev"])

    def test_all_missing(self):
        runner = MagicMock()
        runner.run.return_value = 1
        v = DependencyValidator(runner)
        with patch("builtins.print"):
            ok, missing = v.validate(
                {"apt_deps": ["a", "b"]}
            )
        self.assertFalse(ok)
        self.assertEqual(missing, ["a", "b"])

    def test_prints_install_hint(self):
        runner = MagicMock()
        runner.run.return_value = 1
        v = DependencyValidator(runner)
        printed = []
        with patch(
            "builtins.print",
            side_effect=lambda *a, **kw: (
                printed.append(
                    " ".join(str(x) for x in a)
                )
            ),
        ):
            v.validate({"apt_deps": ["libfoo-dev"]})
        output = "\n".join(printed)
        self.assertIn("apt-get install", output)
        self.assertIn("libfoo-dev", output)


# ============================================================
# RepoCloner
# ============================================================

class TestRepoCloner(unittest.TestCase):
    """Tests for RepoCloner."""

    def test_skips_existing_repo(self):
        with tempfile.TemporaryDirectory() as tmpdir:
            repo_dir = Path(tmpdir) / "myrepo"
            repo_dir.mkdir()
            (repo_dir / "file.txt").touch()

            runner = MagicMock()
            cloner = RepoCloner(runner)
            paths = {"repos_dir": tmpdir}
            cfg = {"url": "x", "branch": "main"}

            with patch("builtins.print"):
                result = cloner.clone(
                    "myrepo", cfg, paths
                )
            self.assertEqual(
                result, str(repo_dir)
            )
            runner.run.assert_not_called()

    def test_clones_new_repo(self):
        with tempfile.TemporaryDirectory() as tmpdir:
            runner = MagicMock()
            runner.run.return_value = 0
            cloner = RepoCloner(runner)
            paths = {"repos_dir": tmpdir}
            cfg = {
                "url": "https://github.com/x/y.git",
                "branch": "main",
            }

            cloner.clone("newrepo", cfg, paths)
            runner.run.assert_called_once()
            call_args = runner.run.call_args
            self.assertIn("git clone", call_args[0][0])
            self.assertIn("main", call_args[0][0])

    def test_default_branch_master(self):
        with tempfile.TemporaryDirectory() as tmpdir:
            runner = MagicMock()
            runner.run.return_value = 0
            cloner = RepoCloner(runner)
            paths = {"repos_dir": tmpdir}
            cfg = {"url": "https://github.com/x/y.git"}

            cloner.clone("repo", cfg, paths)
            call_args = runner.run.call_args
            self.assertIn("master", call_args[0][0])


# ============================================================
# BomtraceBuilder
# ============================================================

class TestBomtraceBuilder(unittest.TestCase):
    """Tests for BomtraceBuilder."""

    def _cfg(self):
        return (
            {
                "build_steps": [
                    "autoreconf -fi",
                    "./configure",
                    "make -j4",
                ],
                "clean_cmd": "make clean",
            },
            {"repos_dir": "/repos", "output_dir": "/out"},
            {
                "tracer": "bomtrace3",
                "raw_logfile": "/tmp/log",
                "create_bom_script": "/usr/bin/bom",
            },
        )

    def test_success(self):
        runner = MagicMock()
        runner.run.return_value = 0
        builder = BomtraceBuilder(runner)
        repo_cfg, paths, omnibor = self._cfg()

        with patch("builtins.print"):
            result = builder.build(
                "curl", repo_cfg, paths, omnibor
            )
        self.assertTrue(result)
        # clean + 2 pre-build + instrumented + ADG = 5
        self.assertEqual(runner.run.call_count, 5)

    def test_success_no_clean_cmd(self):
        runner = MagicMock()
        runner.run.return_value = 0
        builder = BomtraceBuilder(runner)
        repo_cfg, paths, omnibor = self._cfg()
        del repo_cfg["clean_cmd"]

        with patch("builtins.print"):
            result = builder.build(
                "curl", repo_cfg, paths, omnibor
            )
        self.assertTrue(result)
        # no clean + 2 pre-build + instrumented + ADG = 4
        self.assertEqual(runner.run.call_count, 4)

    def test_prebuild_failure(self):
        runner = MagicMock()
        # clean ok, first pre-build fails
        runner.run.side_effect = [0, 1]
        builder = BomtraceBuilder(runner)
        repo_cfg, paths, omnibor = self._cfg()

        with patch("builtins.print"):
            result = builder.build(
                "curl", repo_cfg, paths, omnibor
            )
        self.assertFalse(result)

    def test_make_failure(self):
        runner = MagicMock()
        # clean ok, 2 pre-build ok, instrumented fails
        runner.run.side_effect = [0, 0, 0, 1]
        builder = BomtraceBuilder(runner)
        repo_cfg, paths, omnibor = self._cfg()

        with patch("builtins.print"):
            result = builder.build(
                "curl", repo_cfg, paths, omnibor
            )
        self.assertFalse(result)

    def test_adg_failure(self):
        runner = MagicMock()
        # clean ok, 2 pre-build ok, instrumented ok, ADG fails
        runner.run.side_effect = [0, 0, 0, 0, 1]
        builder = BomtraceBuilder(runner)
        repo_cfg, paths, omnibor = self._cfg()

        with patch("builtins.print"):
            result = builder.build(
                "curl", repo_cfg, paths, omnibor
            )
        self.assertFalse(result)

    def test_clean_failure_ignored(self):
        runner = MagicMock()
        # clean fails (fresh clone), rest succeeds
        runner.run.side_effect = [1, 0, 0, 0, 0]
        builder = BomtraceBuilder(runner)
        repo_cfg, paths, omnibor = self._cfg()

        with patch("builtins.print"):
            result = builder.build(
                "curl", repo_cfg, paths, omnibor
            )
        self.assertTrue(result)

    def test_instrumented_cmd_uses_tracer(self):
        runner = MagicMock()
        runner.run.return_value = 0
        builder = BomtraceBuilder(runner)
        repo_cfg, paths, omnibor = self._cfg()

        with patch("builtins.print"):
            builder.build(
                "curl", repo_cfg, paths, omnibor
            )
        # clean(0) + pre-build(1,2) + instrumented(3)
        instrumented_call = runner.run.call_args_list[3]
        self.assertIn(
            "bomtrace3", instrumented_call[0][0]
        )


# ============================================================
# SpdxGenerator
# ============================================================

class TestSpdxGenerator(unittest.TestCase):
    """Tests for SpdxGenerator."""

    def _setup_repo(self, tmpdir):
        """Create fake repo with binaries."""
        repo_dir = (
            Path(tmpdir) / "repos" / "curl"
            / "src" / ".libs"
        )
        repo_dir.mkdir(parents=True)
        (repo_dir / "curl").write_bytes(b"bin")
        return {
            "repos_dir": str(Path(tmpdir) / "repos"),
            "output_dir": str(
                Path(tmpdir) / "output"
            ),
        }

    def test_generate_calls_bomsh_with_files(self):
        with tempfile.TemporaryDirectory() as td:
            runner = MagicMock()
            runner.run.return_value = 0
            gen = SpdxGenerator(runner)
            paths = self._setup_repo(td)
            repo_cfg = {
                "output_binaries": [
                    "src/.libs/curl"
                ],
            }
            omnibor = {
                "sbom_script": "/usr/bin/sbom",
            }

            with patch("builtins.print"):
                gen.generate(
                    "curl", repo_cfg,
                    paths, omnibor,
                )
            cmd = runner.run.call_args[0][0]
            self.assertIn("-F ", cmd)
            self.assertIn("-s spdx-json", cmd)
            self.assertIn("src/.libs/curl", cmd)

    def test_generate_renames_output(self):
        with tempfile.TemporaryDirectory() as td:
            runner = MagicMock()
            runner.run.return_value = 0
            gen = SpdxGenerator(runner)
            paths = self._setup_repo(td)
            repo_cfg = {
                "output_binaries": [
                    "src/.libs/curl"
                ],
            }
            omnibor = {
                "sbom_script": "/usr/bin/sbom",
            }

            # Simulate bomsh_sbom.py output
            spdx_dir = (
                Path(td) / "output" / "spdx" / "curl"
            )
            spdx_dir.mkdir(parents=True)
            (
                spdx_dir
                / "omnibor.curl.syft.spdx-json"
            ).write_text('{"creationInfo":{}}')

            with patch("builtins.print"), \
                    patch.object(
                        SpdxGenerator,
                        "patch_spdx_metadata",
                    ):
                result = gen.generate(
                    "curl", repo_cfg,
                    paths, omnibor,
                )
            self.assertIsNotNone(result)
            self.assertIn("curl_omnibor_", result)
            self.assertIn(".spdx.json", result)
            self.assertTrue(Path(result).exists())

    def test_generate_no_binaries_returns_none(self):
        with tempfile.TemporaryDirectory() as td:
            runner = MagicMock()
            gen = SpdxGenerator(runner)
            paths = {
                "repos_dir": str(
                    Path(td) / "repos"
                ),
                "output_dir": str(
                    Path(td) / "output"
                ),
            }
            repo_cfg = {
                "output_binaries": [
                    "nonexistent/bin"
                ],
            }
            omnibor = {"sbom_script": "x"}

            with patch("builtins.print"):
                result = gen.generate(
                    "curl", repo_cfg,
                    paths, omnibor,
                )
            self.assertIsNone(result)
            runner.run.assert_not_called()

    def test_generate_warns_on_failure(self):
        with tempfile.TemporaryDirectory() as td:
            runner = MagicMock()
            runner.run.return_value = 1
            gen = SpdxGenerator(runner)
            paths = self._setup_repo(td)
            repo_cfg = {
                "output_binaries": [
                    "src/.libs/curl"
                ],
            }
            omnibor = {"sbom_script": "x"}

            printed = []
            with patch(
                "builtins.print",
                side_effect=lambda *a, **kw: (
                    printed.append(
                        " ".join(str(x) for x in a)
                    )
                ),
            ):
                gen.generate(
                    "curl", repo_cfg,
                    paths, omnibor,
                )
            output = "\n".join(printed)
            self.assertIn("WARN", output)


# ============================================================
# SpdxGenerator — creator patching
# ============================================================

class TestSpdxGeneratorMetadata(unittest.TestCase):
    """Tests for SpdxGenerator.patch_spdx_metadata()."""

    def _write_spdx(self, tmpdir, doc):
        import json
        path = Path(tmpdir) / "test.spdx.json"
        path.write_text(json.dumps(doc))
        return str(path)

    def test_patches_creators_and_namespace(self):
        import json
        with tempfile.TemporaryDirectory() as td:
            doc = {
                "spdxVersion": "SPDX-2.3",
                "name": "curl",
                "documentNamespace": (
                    "https://anchore.com/syft/file/"
                    "curl-a1b2c3d4-e5f6-7890-abcd-ef1234567890"
                ),
                "creationInfo": {
                    "created": "2026-01-01T00:00:00Z",
                    "creators": [
                        "Tool: syft-1.42.0"
                    ],
                },
            }
            path = self._write_spdx(td, doc)
            with patch.object(
                SpdxGenerator, "_bomsh_version",
                return_value="0.0.1-5823f7d",
            ), patch.object(
                SpdxGenerator, "_bomtrace_version",
                return_value="6.11",
            ), patch("builtins.print"):
                ok = SpdxGenerator.patch_spdx_metadata(
                    path
                )

            self.assertTrue(ok)
            result = json.loads(
                Path(path).read_text()
            )
            # --- creators ---
            creators = (
                result["creationInfo"]["creators"]
            )
            self.assertEqual(len(creators), 4)
            self.assertIn(
                "Tool: syft-1.42.0", creators
            )
            self.assertIn(
                "Tool: bomtrace3-6.11", creators
            )
            self.assertIn(
                "Tool: bomsh-0.0.1-5823f7d",
                creators,
            )
            self.assertTrue(
                any(
                    "omnibor-analysis" in c
                    for c in creators
                )
            )
            # --- namespace ---
            ns = result["documentNamespace"]
            self.assertIn("omnibor.io", ns)
            self.assertIn("curl", ns)
            self.assertIn(
                "a1b2c3d4-e5f6-7890-abcd-"
                "ef1234567890",
                ns,
            )
            self.assertNotIn("anchore.com", ns)

    def test_patch_idempotent(self):
        import json
        with tempfile.TemporaryDirectory() as td:
            doc = {
                "spdxVersion": "SPDX-2.3",
                "name": "curl",
                "documentNamespace": (
                    "https://anchore.com/syft/file/"
                    "curl-a1b2c3d4-e5f6-7890-abcd-ef1234567890"
                ),
                "creationInfo": {
                    "created": "2026-01-01T00:00:00Z",
                    "creators": [
                        "Tool: syft-1.42.0"
                    ],
                },
            }
            path = self._write_spdx(td, doc)
            with patch.object(
                SpdxGenerator, "_bomsh_version",
                return_value="0.0.1",
            ), patch.object(
                SpdxGenerator, "_bomtrace_version",
                return_value="6.11",
            ), patch("builtins.print"):
                SpdxGenerator.patch_spdx_metadata(path)
                SpdxGenerator.patch_spdx_metadata(path)

            result = json.loads(
                Path(path).read_text()
            )
            creators = (
                result["creationInfo"]["creators"]
            )
            # Should not duplicate entries
            self.assertEqual(len(creators), 4)

    def test_patch_missing_file(self):
        ok = SpdxGenerator.patch_spdx_metadata(
            "/nonexistent.json"
        )
        self.assertFalse(ok)

    def test_patch_invalid_json(self):
        with tempfile.NamedTemporaryFile(
            suffix=".json", mode="w", delete=False,
        ) as f:
            f.write("not json{{{")
            path = f.name
        try:
            ok = SpdxGenerator.patch_spdx_metadata(
                path
            )
            self.assertFalse(ok)
        finally:
            Path(path).unlink()

    def test_patch_no_creation_info(self):
        import json
        with tempfile.TemporaryDirectory() as td:
            doc = {"spdxVersion": "SPDX-2.3"}
            path = self._write_spdx(td, doc)
            ok = SpdxGenerator.patch_spdx_metadata(
                path
            )
            self.assertFalse(ok)

    def test_namespace_no_uuid_uses_timestamp(self):
        import json
        with tempfile.TemporaryDirectory() as td:
            doc = {
                "spdxVersion": "SPDX-2.3",
                "name": "curl",
                "documentNamespace": (
                    "https://example.com/no-uuid"
                ),
                "creationInfo": {
                    "created": "2026-01-01T00:00:00Z",
                    "creators": [],
                },
            }
            path = self._write_spdx(td, doc)
            with patch.object(
                SpdxGenerator, "_bomsh_version",
                return_value="0.0.1",
            ), patch.object(
                SpdxGenerator, "_bomtrace_version",
                return_value="6.11",
            ), patch(
                "analyze.timestamp",
                return_value="2026-02-12_1300",
            ), patch("builtins.print"):
                ok = SpdxGenerator.patch_spdx_metadata(
                    path
                )
            self.assertTrue(ok)
            result = json.loads(
                Path(path).read_text()
            )
            ns = result["documentNamespace"]
            self.assertIn("omnibor.io", ns)
            self.assertIn(
                "2026-02-12_1300", ns
            )

    def test_bomsh_version_fallback(self):
        with patch(
            "subprocess.check_output",
            side_effect=Exception("no cmd"),
        ):
            ver = SpdxGenerator._bomsh_version()
        self.assertEqual(ver, "unknown")

    def test_bomtrace_version_fallback(self):
        with patch(
            "subprocess.check_output",
            side_effect=Exception("no cmd"),
        ):
            ver = SpdxGenerator._bomtrace_version()
        self.assertEqual(ver, "unknown")

    def test_generate_calls_patch_on_success(self):
        with tempfile.TemporaryDirectory() as td:
            # Set up repo with binary
            repo_dir = (
                Path(td) / "repos" / "curl"
                / "src" / ".libs"
            )
            repo_dir.mkdir(parents=True)
            (repo_dir / "curl").write_bytes(b"bin")
            paths = {
                "repos_dir": str(
                    Path(td) / "repos"
                ),
                "output_dir": str(
                    Path(td) / "output"
                ),
            }
            repo_cfg = {
                "output_binaries": [
                    "src/.libs/curl"
                ],
            }
            omnibor = {
                "sbom_script": "/usr/bin/sbom",
            }
            # Simulate bomsh output
            spdx_dir = (
                Path(td) / "output"
                / "spdx" / "curl"
            )
            spdx_dir.mkdir(parents=True)
            (
                spdx_dir
                / "omnibor.curl.syft.spdx-json"
            ).write_text('{"creationInfo":{}}')

            runner = MagicMock()
            runner.run.return_value = 0
            gen = SpdxGenerator(runner)
            with patch.object(
                SpdxGenerator,
                "patch_spdx_metadata",
            ) as mock_patch, patch(
                "builtins.print"
            ):
                result = gen.generate(
                    "curl", repo_cfg,
                    paths, omnibor,
                )
            bom_dir = str(
                Path(td) / "output"
                / "omnibor" / "curl"
            )
            mock_patch.assert_called_once_with(
                result, bom_dir
            )

    def test_generate_no_patch_when_no_output(self):
        with tempfile.TemporaryDirectory() as td:
            repo_dir = (
                Path(td) / "repos" / "curl"
                / "src" / ".libs"
            )
            repo_dir.mkdir(parents=True)
            (repo_dir / "curl").write_bytes(b"bin")
            paths = {
                "repos_dir": str(
                    Path(td) / "repos"
                ),
                "output_dir": str(
                    Path(td) / "output"
                ),
            }
            repo_cfg = {
                "output_binaries": [
                    "src/.libs/curl"
                ],
            }
            omnibor = {"sbom_script": "x"}
            runner = MagicMock()
            runner.run.return_value = 1
            gen = SpdxGenerator(runner)
            with patch.object(
                SpdxGenerator,
                "patch_spdx_metadata",
            ) as mock_patch, patch(
                "builtins.print"
            ):
                result = gen.generate(
                    "curl", repo_cfg,
                    paths, omnibor,
                )
            # No bomsh output file, so no patch
            mock_patch.assert_not_called()
            self.assertIsNone(result)

    def test_inject_omnibor_refs(self):
        """ExternalRefs injected when logfile+mapping exist."""
        import json
        with tempfile.TemporaryDirectory() as td:
            # Create bomsh metadata
            meta = (
                Path(td) / "bom" / "metadata" / "bomsh"
            )
            meta.mkdir(parents=True)
            sha_curl = "a" * 40
            sha_lib = "b" * 40
            (meta / "bomsh_hook_raw_logfile").write_text(
                f"outfile: {sha_curl} path: /repo/src/.libs/curl\n"
                f"outfile: {sha_lib} path: /repo/lib/.libs/libcurl.so\n"
            )
            (meta / "bomsh_omnibor_doc_mapping").write_text(
                json.dumps({
                    sha_curl: "omnibor_doc_curl",
                    sha_lib: "omnibor_doc_libcurl",
                })
            )
            # SPDX doc with matching packages
            doc = {
                "spdxVersion": "SPDX-2.3",
                "name": "curl",
                "documentNamespace": (
                    "https://anchore.com/syft/"
                    "curl-a1b2c3d4-e5f6-7890-"
                    "abcd-ef1234567890"
                ),
                "creationInfo": {
                    "created": "2026-01-01T00:00:00Z",
                    "creators": ["Tool: syft-1.42.0"],
                },
                "packages": [
                    {
                        "name": "curl",
                        "SPDXID": "SPDXRef-curl",
                        "externalRefs": [],
                    },
                ],
            }
            path = self._write_spdx(td, doc)
            with patch.object(
                SpdxGenerator, "_bomsh_version",
                return_value="0.0.1",
            ), patch.object(
                SpdxGenerator, "_bomtrace_version",
                return_value="6.11",
            ), patch("builtins.print"):
                ok = SpdxGenerator.patch_spdx_metadata(
                    path, str(Path(td) / "bom")
                )
            self.assertTrue(ok)
            result = json.loads(
                Path(path).read_text()
            )
            refs = result["packages"][0][
                "externalRefs"
            ]
            omnibor_refs = [
                r for r in refs
                if "gitoid" in r.get(
                    "referenceLocator", ""
                )
            ]
            self.assertEqual(len(omnibor_refs), 1)
            self.assertIn(
                "omnibor_doc_curl",
                omnibor_refs[0]["referenceLocator"],
            )

    def test_inject_omnibor_refs_no_metadata(self):
        """No crash when bom_dir has no metadata."""
        import json
        with tempfile.TemporaryDirectory() as td:
            doc = {
                "spdxVersion": "SPDX-2.3",
                "name": "curl",
                "documentNamespace": (
                    "https://anchore.com/syft/"
                    "curl-a1b2c3d4-e5f6-7890-"
                    "abcd-ef1234567890"
                ),
                "creationInfo": {
                    "created": "2026-01-01T00:00:00Z",
                    "creators": [],
                },
                "packages": [
                    {"name": "curl", "SPDXID": "x"},
                ],
            }
            path = self._write_spdx(td, doc)
            bom_dir = str(Path(td) / "empty_bom")
            with patch.object(
                SpdxGenerator, "_bomsh_version",
                return_value="0.0.1",
            ), patch.object(
                SpdxGenerator, "_bomtrace_version",
                return_value="6.11",
            ), patch("builtins.print"):
                ok = SpdxGenerator.patch_spdx_metadata(
                    path, bom_dir
                )
            self.assertTrue(ok)
            result = json.loads(
                Path(path).read_text()
            )
            # No ExternalRefs injected
            refs = result["packages"][0].get(
                "externalRefs", []
            )
            omnibor = [
                r for r in refs
                if "gitoid" in r.get(
                    "referenceLocator", ""
                )
            ]
            self.assertEqual(len(omnibor), 0)

    def test_inject_omnibor_refs_no_match(self):
        """No injection when package name doesn't match."""
        import json
        with tempfile.TemporaryDirectory() as td:
            meta = (
                Path(td) / "bom" / "metadata" / "bomsh"
            )
            meta.mkdir(parents=True)
            sha_other = "c" * 40
            (meta / "bomsh_hook_raw_logfile").write_text(
                f"outfile: {sha_other} path: /repo/other_bin\n"
            )
            (meta / "bomsh_omnibor_doc_mapping").write_text(
                json.dumps({sha_other: "doc_other"})
            )
            doc = {
                "spdxVersion": "SPDX-2.3",
                "name": "curl",
                "documentNamespace": (
                    "https://anchore.com/syft/"
                    "curl-a1b2c3d4-e5f6-7890-"
                    "abcd-ef1234567890"
                ),
                "creationInfo": {
                    "created": "2026-01-01T00:00:00Z",
                    "creators": [],
                },
                "packages": [
                    {"name": "curl", "SPDXID": "x"},
                ],
            }
            path = self._write_spdx(td, doc)
            with patch.object(
                SpdxGenerator, "_bomsh_version",
                return_value="0.0.1",
            ), patch.object(
                SpdxGenerator, "_bomtrace_version",
                return_value="6.11",
            ), patch("builtins.print"):
                ok = SpdxGenerator.patch_spdx_metadata(
                    path, str(Path(td) / "bom")
                )
            self.assertTrue(ok)
            result = json.loads(
                Path(path).read_text()
            )
            refs = result["packages"][0].get(
                "externalRefs", []
            )
            self.assertEqual(len(refs), 0)

    def test_inject_refs_bad_mapping_json(self):
        """Graceful when mapping file has invalid JSON."""
        import json
        with tempfile.TemporaryDirectory() as td:
            meta = (
                Path(td) / "bom" / "metadata" / "bomsh"
            )
            meta.mkdir(parents=True)
            sha = "d" * 40
            (meta / "bomsh_hook_raw_logfile").write_text(
                f"outfile: {sha} path: /repo/curl\n"
            )
            (meta / "bomsh_omnibor_doc_mapping").write_text(
                "not-json{{{"
            )
            doc = {
                "spdxVersion": "SPDX-2.3",
                "name": "curl",
                "documentNamespace": (
                    "https://anchore.com/syft/"
                    "curl-a1b2c3d4-e5f6-7890-"
                    "abcd-ef1234567890"
                ),
                "creationInfo": {
                    "created": "2026-01-01T00:00:00Z",
                    "creators": [],
                },
                "packages": [
                    {"name": "curl", "SPDXID": "x"},
                ],
            }
            path = self._write_spdx(td, doc)
            with patch.object(
                SpdxGenerator, "_bomsh_version",
                return_value="0.0.1",
            ), patch.object(
                SpdxGenerator, "_bomtrace_version",
                return_value="6.11",
            ), patch("builtins.print"):
                ok = SpdxGenerator.patch_spdx_metadata(
                    path, str(Path(td) / "bom")
                )
            self.assertTrue(ok)

    def test_inject_refs_logfile_only(self):
        """Graceful when mapping file is missing."""
        import json
        with tempfile.TemporaryDirectory() as td:
            meta = (
                Path(td) / "bom" / "metadata" / "bomsh"
            )
            meta.mkdir(parents=True)
            sha = "e" * 40
            (meta / "bomsh_hook_raw_logfile").write_text(
                f"outfile: {sha} path: /repo/curl\n"
            )
            # No mapping file
            doc = {
                "spdxVersion": "SPDX-2.3",
                "name": "curl",
                "documentNamespace": (
                    "https://anchore.com/syft/"
                    "curl-a1b2c3d4-e5f6-7890-"
                    "abcd-ef1234567890"
                ),
                "creationInfo": {
                    "created": "2026-01-01T00:00:00Z",
                    "creators": [],
                },
                "packages": [
                    {"name": "curl", "SPDXID": "x"},
                ],
            }
            path = self._write_spdx(td, doc)
            with patch.object(
                SpdxGenerator, "_bomsh_version",
                return_value="0.0.1",
            ), patch.object(
                SpdxGenerator, "_bomtrace_version",
                return_value="6.11",
            ), patch("builtins.print"):
                ok = SpdxGenerator.patch_spdx_metadata(
                    path, str(Path(td) / "bom")
                )
            self.assertTrue(ok)

    def test_inject_refs_hash_not_in_mapping(self):
        """No ref when hash exists in logfile but not mapping."""
        import json
        with tempfile.TemporaryDirectory() as td:
            meta = (
                Path(td) / "bom" / "metadata" / "bomsh"
            )
            meta.mkdir(parents=True)
            sha = "f" * 40
            (meta / "bomsh_hook_raw_logfile").write_text(
                f"outfile: {sha} path: /repo/curl\n"
            )
            (meta / "bomsh_omnibor_doc_mapping").write_text(
                json.dumps({"other_hash": "doc"})
            )
            doc = {
                "spdxVersion": "SPDX-2.3",
                "name": "curl",
                "documentNamespace": (
                    "https://anchore.com/syft/"
                    "curl-a1b2c3d4-e5f6-7890-"
                    "abcd-ef1234567890"
                ),
                "creationInfo": {
                    "created": "2026-01-01T00:00:00Z",
                    "creators": [],
                },
                "packages": [
                    {"name": "curl", "SPDXID": "x"},
                ],
            }
            path = self._write_spdx(td, doc)
            with patch.object(
                SpdxGenerator, "_bomsh_version",
                return_value="0.0.1",
            ), patch.object(
                SpdxGenerator, "_bomtrace_version",
                return_value="6.11",
            ), patch("builtins.print"):
                ok = SpdxGenerator.patch_spdx_metadata(
                    path, str(Path(td) / "bom")
                )
            self.assertTrue(ok)
            result = json.loads(
                Path(path).read_text()
            )
            refs = result["packages"][0].get(
                "externalRefs", []
            )
            self.assertEqual(len(refs), 0)


# ============================================================
# SpdxValidator
# ============================================================

class TestSpdxValidator(unittest.TestCase):
    """Tests for SpdxValidator."""

    def _minimal_spdx(self):
        """Return a minimal valid SPDX 2.3 JSON dict."""
        return {
            "spdxVersion": "SPDX-2.3",
            "dataLicense": "CC0-1.0",
            "SPDXID": "SPDXRef-DOCUMENT",
            "name": "test-doc",
            "documentNamespace": (
                "https://example.org/test"
            ),
            "creationInfo": {
                "created": "2026-01-01T00:00:00Z",
                "creators": ["Tool: test"],
            },
        }

    def test_validate_file_not_found(self):
        v = SpdxValidator()
        printed = []
        with patch(
            "builtins.print",
            side_effect=lambda *a, **kw: (
                printed.append(
                    " ".join(str(x) for x in a)
                )
            ),
        ):
            result = v.validate("/nonexistent.json")
        self.assertIsNone(result["schema_ok"])
        self.assertIsNone(result["semantic_ok"])
        self.assertIn(
            "not found",
            "\n".join(printed),
        )

    def test_validate_invalid_json(self):
        import json
        v = SpdxValidator()
        with tempfile.NamedTemporaryFile(
            suffix=".spdx.json", mode="w",
            delete=False,
        ) as f:
            f.write("not json{{{")
            path = f.name
        try:
            printed = []
            with patch(
                "builtins.print",
                side_effect=lambda *a, **kw: (
                    printed.append(
                        " ".join(str(x) for x in a)
                    )
                ),
            ):
                result = v.validate(path)
            self.assertIsNone(result["schema_ok"])
            output = "\n".join(printed)
            self.assertIn("Cannot read", output)
        finally:
            Path(path).unlink()

    def test_schema_validation_pass(self):
        """Minimal SPDX doc should pass schema."""
        import json
        v = SpdxValidator()
        with tempfile.NamedTemporaryFile(
            suffix=".spdx.json", mode="w",
            delete=False,
        ) as f:
            json.dump(self._minimal_spdx(), f)
            path = f.name
        try:
            with patch("builtins.print"):
                result = v.validate(path)
            # Schema should pass (or be skipped if
            # network unavailable)
            if result["schema_ok"] is not None:
                self.assertTrue(result["schema_ok"])
        finally:
            Path(path).unlink()

    def test_schema_validation_fail(self):
        """Invalid doc should fail schema."""
        import json
        v = SpdxValidator()
        bad_doc = {"not": "spdx"}
        with tempfile.NamedTemporaryFile(
            suffix=".spdx.json", mode="w",
            delete=False,
        ) as f:
            json.dump(bad_doc, f)
            path = f.name
        try:
            with patch("builtins.print"):
                result = v.validate(path)
            if result["schema_ok"] is not None:
                self.assertFalse(result["schema_ok"])
                self.assertTrue(
                    len(result["schema_errors"]) > 0
                )
        finally:
            Path(path).unlink()

    def test_schema_skipped_when_jsonschema_missing(
        self,
    ):
        """Schema check skipped if jsonschema absent."""
        import json
        v = SpdxValidator()
        with tempfile.NamedTemporaryFile(
            suffix=".spdx.json", mode="w",
            delete=False,
        ) as f:
            json.dump(self._minimal_spdx(), f)
            path = f.name
        try:
            import builtins
            real_import = builtins.__import__

            def mock_import(name, *args, **kwargs):
                if name == "jsonschema":
                    raise ImportError("mocked")
                return real_import(
                    name, *args, **kwargs
                )

            with patch(
                "builtins.__import__",
                side_effect=mock_import,
            ):
                with patch("builtins.print"):
                    result = v.validate(path)
            self.assertIsNone(result["schema_ok"])
        finally:
            Path(path).unlink()

    def test_semantic_skipped_when_spdx_tools_missing(
        self,
    ):
        """Semantic check skipped if spdx-tools absent."""
        import json
        v = SpdxValidator()
        with tempfile.NamedTemporaryFile(
            suffix=".spdx.json", mode="w",
            delete=False,
        ) as f:
            json.dump(self._minimal_spdx(), f)
            path = f.name
        try:
            import builtins
            real_import = builtins.__import__

            def mock_import(name, *args, **kwargs):
                if "spdx_tools" in name:
                    raise ImportError("mocked")
                return real_import(
                    name, *args, **kwargs
                )

            with patch(
                "builtins.__import__",
                side_effect=mock_import,
            ):
                with patch("builtins.print"):
                    result = v.validate(path)
            self.assertIsNone(result["semantic_ok"])
        finally:
            Path(path).unlink()

    def test_semantic_parse_error(self):
        """Semantic fails gracefully on unparseable doc."""
        import json
        v = SpdxValidator()
        bad_doc = {"spdxVersion": "SPDX-2.3"}
        with tempfile.NamedTemporaryFile(
            suffix=".spdx.json", mode="w",
            delete=False,
        ) as f:
            json.dump(bad_doc, f)
            path = f.name
        try:
            with patch("builtins.print"):
                result = v.validate(path)
            if result["semantic_ok"] is not None:
                self.assertFalse(result["semantic_ok"])
                self.assertTrue(
                    len(result["semantic_errors"]) > 0
                )
        finally:
            Path(path).unlink()

    def test_print_summary_pass(self):
        """Summary prints PASS for valid results."""
        printed = []
        with patch(
            "builtins.print",
            side_effect=lambda *a, **kw: (
                printed.append(
                    " ".join(str(x) for x in a)
                )
            ),
        ):
            SpdxValidator._print_summary(
                "/tmp/test.spdx.json",
                {
                    "schema_ok": True,
                    "semantic_ok": True,
                    "schema_errors": [],
                    "semantic_errors": [],
                },
            )
        output = "\n".join(printed)
        self.assertIn("PASS", output)

    def test_print_summary_fail(self):
        """Summary prints FAIL with error count."""
        printed = []
        with patch(
            "builtins.print",
            side_effect=lambda *a, **kw: (
                printed.append(
                    " ".join(str(x) for x in a)
                )
            ),
        ):
            SpdxValidator._print_summary(
                "/tmp/test.spdx.json",
                {
                    "schema_ok": False,
                    "semantic_ok": False,
                    "schema_errors": ["err1", "err2"],
                    "semantic_errors": ["err3"],
                },
            )
        output = "\n".join(printed)
        self.assertIn("FAIL", output)
        self.assertIn("2 errors", output)

    def test_print_summary_skipped(self):
        """Summary prints SKIPPED when None."""
        printed = []
        with patch(
            "builtins.print",
            side_effect=lambda *a, **kw: (
                printed.append(
                    " ".join(str(x) for x in a)
                )
            ),
        ):
            SpdxValidator._print_summary(
                "/tmp/test.spdx.json",
                {
                    "schema_ok": None,
                    "semantic_ok": None,
                    "schema_errors": [],
                    "semantic_errors": [],
                },
            )
        output = "\n".join(printed)
        self.assertIn("SKIPPED", output)


# ============================================================
# SyftGenerator
# ============================================================

class TestSyftGenerator(unittest.TestCase):
    """Tests for SyftGenerator."""

    def test_generate(self):
        with tempfile.TemporaryDirectory() as tmpdir:
            runner = MagicMock()
            runner.run.return_value = 0
            gen = SyftGenerator(runner)
            paths = {
                "repos_dir": tmpdir,
                "output_dir": tmpdir,
            }

            with patch("builtins.print"):
                result = gen.generate("curl", paths)
            self.assertIn("curl_syft_", result)
            self.assertIn(".spdx.json", result)

    def test_generate_warns_on_failure(self):
        with tempfile.TemporaryDirectory() as tmpdir:
            runner = MagicMock()
            runner.run.return_value = 1
            gen = SyftGenerator(runner)
            paths = {
                "repos_dir": tmpdir,
                "output_dir": tmpdir,
            }

            printed = []
            with patch(
                "builtins.print",
                side_effect=lambda *a, **kw: (
                    printed.append(
                        " ".join(str(x) for x in a)
                    )
                ),
            ):
                gen.generate("curl", paths)
            output = "\n".join(printed)
            self.assertIn("WARN", output)


# ============================================================
# BinaryCollector
# ============================================================

class TestBinaryCollector(unittest.TestCase):
    """Tests for BinaryCollector."""

    @patch("analyze.timestamp", return_value="2026-02-12_1300")
    def test_collect_copies_binaries(self, _ts):
        with tempfile.TemporaryDirectory() as tmpdir:
            # Create fake repo with a binary
            repo_dir = Path(tmpdir) / "repos" / "curl"
            (repo_dir / "src" / ".libs").mkdir(
                parents=True
            )
            binary = repo_dir / "src" / ".libs" / "curl"
            binary.write_bytes(b"\x7fELF fake binary")

            paths = {
                "repos_dir": str(Path(tmpdir) / "repos"),
                "output_dir": str(
                    Path(tmpdir) / "output"
                ),
            }
            cfg = {
                "output_binaries": [
                    "src/.libs/curl"
                ],
            }

            with patch("builtins.print"):
                result = BinaryCollector.collect(
                    "curl", cfg, paths
                )

            self.assertEqual(len(result), 1)
            dst = Path(result[0][1])
            self.assertTrue(dst.exists())
            self.assertEqual(dst.name, "curl")
            self.assertIn(
                "2026-02-12_1300", str(dst)
            )
            self.assertEqual(
                dst.read_bytes(),
                b"\x7fELF fake binary",
            )

    @patch("analyze.timestamp", return_value="2026-02-12_1300")
    def test_collect_missing_binary_warns(self, _ts):
        with tempfile.TemporaryDirectory() as tmpdir:
            repo_dir = Path(tmpdir) / "repos" / "curl"
            repo_dir.mkdir(parents=True)

            paths = {
                "repos_dir": str(Path(tmpdir) / "repos"),
                "output_dir": str(
                    Path(tmpdir) / "output"
                ),
            }
            cfg = {
                "output_binaries": [
                    "src/.libs/curl"
                ],
            }

            printed = []
            with patch(
                "builtins.print",
                side_effect=lambda *a, **kw: (
                    printed.append(
                        " ".join(str(x) for x in a)
                    )
                ),
            ):
                result = BinaryCollector.collect(
                    "curl", cfg, paths
                )

            self.assertEqual(len(result), 0)
            output = "\n".join(printed)
            self.assertIn("not found", output)

    def test_collect_no_output_binaries_defined(self):
        paths = {
            "repos_dir": "/tmp",
            "output_dir": "/tmp",
        }
        cfg = {}

        printed = []
        with patch(
            "builtins.print",
            side_effect=lambda *a, **kw: (
                printed.append(
                    " ".join(str(x) for x in a)
                )
            ),
        ):
            result = BinaryCollector.collect(
                "curl", cfg, paths
            )

        self.assertEqual(len(result), 0)
        output = "\n".join(printed)
        self.assertIn("No output_binaries", output)

    @patch("analyze.timestamp", return_value="2026-02-12_1300")
    def test_collect_multiple_binaries(self, _ts):
        with tempfile.TemporaryDirectory() as tmpdir:
            repo_dir = Path(tmpdir) / "repos" / "curl"
            (repo_dir / "src" / ".libs").mkdir(
                parents=True
            )
            (repo_dir / "lib" / ".libs").mkdir(
                parents=True
            )
            (
                repo_dir / "src" / ".libs" / "curl"
            ).write_bytes(b"bin1")
            (
                repo_dir / "lib" / ".libs"
                / "libcurl.so"
            ).write_bytes(b"bin2")

            paths = {
                "repos_dir": str(Path(tmpdir) / "repos"),
                "output_dir": str(
                    Path(tmpdir) / "output"
                ),
            }
            cfg = {
                "output_binaries": [
                    "src/.libs/curl",
                    "lib/.libs/libcurl.so",
                ],
            }

            with patch("builtins.print"):
                result = BinaryCollector.collect(
                    "curl", cfg, paths
                )

            self.assertEqual(len(result), 2)
            out_dir = (
                Path(tmpdir) / "output"
                / "binaries" / "curl"
                / "2026-02-12_1300"
            )
            self.assertTrue(
                (out_dir / "curl").exists()
            )
            self.assertTrue(
                (out_dir / "libcurl.so").exists()
            )


# ============================================================
# DocWriter
# ============================================================

class TestDocWriter(unittest.TestCase):
    """Tests for DocWriter."""

    def test_write_build_doc_success(self):
        with tempfile.TemporaryDirectory() as tmpdir:
            paths = {"docs_dir": tmpdir}
            cfg = {
                "url": "https://github.com/x/y.git",
                "branch": "main",
                "description": "test repo",
                "build_steps": ["./configure", "make"],
                "output_binaries": ["bin/app"],
            }
            with patch("builtins.print"):
                result = DocWriter.write_build_doc(
                    "myrepo", cfg, paths,
                    True, 42.5,
                )
            self.assertTrue(Path(result).exists())
            content = Path(result).read_text()
            self.assertIn("SUCCESS", content)
            self.assertIn("42.5", content)
            self.assertIn("myrepo", content)
            self.assertIn("bin/app", content)

    def test_write_build_doc_failure(self):
        with tempfile.TemporaryDirectory() as tmpdir:
            paths = {"docs_dir": tmpdir}
            cfg = {
                "url": "x",
                "build_steps": ["make"],
            }
            with patch("builtins.print"):
                result = DocWriter.write_build_doc(
                    "repo", cfg, paths,
                    False, 10.0,
                )
            content = Path(result).read_text()
            self.assertIn("FAILED", content)

    def test_write_runtime_doc(self):
        with tempfile.TemporaryDirectory() as tmpdir:
            paths = {"docs_dir": tmpdir}
            with patch("builtins.print"):
                result = DocWriter.write_runtime_doc(
                    "myrepo", paths, 55.3,
                )
            self.assertTrue(Path(result).exists())
            content = Path(result).read_text()
            self.assertIn("55.3", content)
            self.assertIn("myrepo", content)

    def test_write_runtime_doc_with_baseline(self):
        with tempfile.TemporaryDirectory() as tmpdir:
            paths = {"docs_dir": tmpdir}
            with patch("builtins.print"):
                result = DocWriter.write_runtime_doc(
                    "repo", paths, 60.0,
                    baseline_sec=30.0,
                )
            content = Path(result).read_text()
            self.assertIn("100.0%", content)

    def test_write_runtime_doc_no_baseline(self):
        with tempfile.TemporaryDirectory() as tmpdir:
            paths = {"docs_dir": tmpdir}
            with patch("builtins.print"):
                result = DocWriter.write_runtime_doc(
                    "repo", paths, 60.0,
                    baseline_sec=None,
                )
            content = Path(result).read_text()
            self.assertNotIn("overhead", content)


# ============================================================
# AnalysisPipeline
# ============================================================

class TestAnalysisPipeline(unittest.TestCase):
    """Tests for AnalysisPipeline facade."""

    def test_default_construction(self):
        p = AnalysisPipeline()
        self.assertIsInstance(p.runner, CommandRunner)
        self.assertIsInstance(
            p.validator, DependencyValidator
        )
        self.assertIsInstance(p.cloner, RepoCloner)
        self.assertIsInstance(
            p.builder, BomtraceBuilder
        )
        self.assertIsInstance(
            p.spdx_gen, SpdxGenerator
        )
        self.assertIsInstance(
            p.spdx_validator, SpdxValidator
        )
        self.assertIsInstance(
            p.syft_gen, SyftGenerator
        )
        self.assertIsInstance(
            p.binary_collector, BinaryCollector
        )
        self.assertIsInstance(p.docs, DocWriter)

    def test_injected_components(self):
        runner = MagicMock()
        p = AnalysisPipeline(runner=runner)
        self.assertIs(p.runner, runner)

    def test_list_repos(self):
        config = {
            "repos": {
                "curl": {
                    "description": "URL lib",
                    "url": "https://github.com/curl/curl.git",
                },
            }
        }
        printed = []
        with patch(
            "builtins.print",
            side_effect=lambda *a, **kw: (
                printed.append(
                    " ".join(str(x) for x in a)
                )
            ),
        ):
            AnalysisPipeline.list_repos(config)
        output = "\n".join(printed)
        self.assertIn("curl", output)
        self.assertIn("URL lib", output)


# ============================================================
# main() CLI
# ============================================================

def _mock_pipeline():
    """Create an AnalysisPipeline with all mocked components."""
    runner = MagicMock()
    validator = MagicMock()
    validator.validate.return_value = (True, [])
    cloner = MagicMock()
    builder = MagicMock()
    spdx_gen = MagicMock()
    spdx_validator = MagicMock()
    syft_gen = MagicMock()
    binary_collector = MagicMock()
    doc_writer = MagicMock()
    return AnalysisPipeline(
        runner=runner,
        validator=validator,
        cloner=cloner,
        builder=builder,
        spdx_gen=spdx_gen,
        spdx_validator=spdx_validator,
        syft_gen=syft_gen,
        binary_collector=binary_collector,
        doc_writer=doc_writer,
    )


class TestMainList(unittest.TestCase):
    """Tests for main() --list mode."""

    @patch("analyze.AnalysisPipeline")
    @patch("sys.argv", ["analyze.py", "--list"])
    def test_list_mode(self, mock_cls):
        p = MagicMock()
        mock_cls.return_value = p
        with patch("builtins.print"):
            analyze.main()
        p.list_repos.assert_called_once()


class TestMainNoRepo(unittest.TestCase):
    """Tests for main() without --repo."""

    @patch("analyze.AnalysisPipeline")
    @patch("sys.argv", ["analyze.py"])
    def test_exits_without_repo(self, mock_cls):
        mock_cls.return_value = _mock_pipeline()
        with patch("builtins.print"):
            with self.assertRaises(
                SystemExit
            ) as cm:
                analyze.main()
            self.assertEqual(cm.exception.code, 1)


class TestMainUnknownRepo(unittest.TestCase):
    """Tests for main() with unknown repo."""

    @patch("analyze.AnalysisPipeline")
    @patch(
        "sys.argv",
        ["analyze.py", "--repo", "nonexistent"],
    )
    def test_exits_unknown_repo(self, mock_cls):
        mock_cls.return_value = _mock_pipeline()
        with patch("builtins.print"):
            with self.assertRaises(
                SystemExit
            ) as cm:
                analyze.main()
            self.assertEqual(cm.exception.code, 1)


class TestMainFullRun(unittest.TestCase):
    """Tests for main() full analysis run."""

    @patch("analyze.time.time")
    @patch("analyze.AnalysisPipeline")
    @patch(
        "sys.argv",
        ["analyze.py", "--repo", "curl"],
    )
    def test_full_run_success(
        self, mock_cls, mock_time
    ):
        p = _mock_pipeline()
        mock_cls.return_value = p
        p.builder.build.return_value = True
        mock_time.side_effect = [100.0, 142.5]

        with patch("builtins.print"):
            analyze.main()

        p.cloner.clone.assert_called_once()
        p.syft_gen.generate.assert_called_once()
        p.validator.validate.assert_called_once()
        p.builder.build.assert_called_once()
        p.spdx_gen.generate.assert_called_once()
        p.spdx_validator.validate.assert_called_once()
        p.binary_collector.collect.assert_called_once()
        p.docs.write_build_doc.assert_called_once()
        p.docs.write_runtime_doc.assert_called_once()

    @patch("analyze.AnalysisPipeline")
    @patch(
        "sys.argv",
        ["analyze.py", "--repo", "curl"],
    )
    def test_validation_failure_exits(
        self, mock_cls
    ):
        p = _mock_pipeline()
        mock_cls.return_value = p
        p.validator.validate.return_value = (
            False, ["libpsl-dev"]
        )

        with patch("builtins.print"):
            with self.assertRaises(
                SystemExit
            ) as cm:
                analyze.main()
            self.assertEqual(cm.exception.code, 1)

        p.builder.build.assert_not_called()

    @patch("analyze.time.time")
    @patch("analyze.AnalysisPipeline")
    @patch(
        "sys.argv",
        ["analyze.py", "--repo", "curl"],
    )
    def test_full_run_build_failure(
        self, mock_cls, mock_time
    ):
        p = _mock_pipeline()
        mock_cls.return_value = p
        p.builder.build.return_value = False
        mock_time.side_effect = [100.0, 110.0]

        with patch("builtins.print"):
            analyze.main()

        p.spdx_gen.generate.assert_not_called()
        p.spdx_validator.validate.assert_not_called()
        p.binary_collector.collect.assert_not_called()
        p.docs.write_build_doc.assert_called_once()

    @patch("analyze.time.time")
    @patch("analyze.AnalysisPipeline")
    @patch(
        "sys.argv",
        [
            "analyze.py", "--repo", "curl",
            "--skip-clone",
        ],
    )
    def test_skip_clone(self, mock_cls, mock_time):
        p = _mock_pipeline()
        mock_cls.return_value = p
        p.builder.build.return_value = True
        mock_time.side_effect = [100.0, 110.0]

        with patch("builtins.print"):
            analyze.main()

        p.cloner.clone.assert_not_called()

    @patch("analyze.AnalysisPipeline")
    @patch(
        "sys.argv",
        [
            "analyze.py", "--repo", "curl",
            "--syft-only",
        ],
    )
    def test_syft_only(self, mock_cls):
        p = _mock_pipeline()
        mock_cls.return_value = p

        with patch("builtins.print"):
            analyze.main()

        p.syft_gen.generate.assert_called_once()
        p.builder.build.assert_not_called()
        p.spdx_gen.generate.assert_not_called()


if __name__ == "__main__":
    unittest.main()
